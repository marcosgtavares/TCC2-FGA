{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39400327-13f4-46b9-9ad5-2d8f6c8a8434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 04:13:47.655120: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-01 04:13:47.655211: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-01 04:13:47.712722: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-01 04:13:47.840041: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-01 04:13:49.062702: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##FONTE https://github.com/sonisanskar/TextConvoNet/blob/main/TextConvoNet.py\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential,Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM,SpatialDropout1D,Reshape\n",
    "from tensorflow.keras.layers import Embedding,concatenate\n",
    "from tensorflow.keras.layers import Conv2D, GlobalMaxPooling2D,MaxPool2D,MaxPool3D,GlobalAveragePooling2D,Conv3D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e746e08d-ef31-4e76-a5ae-92274ab91adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/283557320663357042', creation_time=1716507639562, experiment_id='283557320663357042', last_update_time=1716507639562, lifecycle_stage='active', name='FAKE_CNN', tags={'mlflow.sharedViewState.7f4d6bce474cd5aa31ae55b20571347b8815f24e8a0cb742cae269d83cfa7446': '{\"searchFilter\":\"\",\"orderByKey\":\"attributes.start_time\",\"orderByAsc\":false,\"startTime\":\"ALL\",\"lifecycleFilter\":\"Active\",\"datasetsFilter\":[],\"modelVersionFilter\":\"All '\n",
       "                                                                                            'Runs\",\"selectedColumns\":[\"attributes.`Source`\",\"attributes.`Models`\",\"attributes.`Dataset`\"],\"runsPinned\":[],\"runsHidden\":[],\"runsHiddenMode\":\"FIRST_10_RUNS\",\"compareRunCharts\":[{\"uuid\":\"17165424146308eouiehb\",\"type\":\"BAR\",\"runsCountToCompare\":10,\"metricSectionId\":\"1716542414630z4tttoib\",\"deleted\":false,\"isGenerated\":true,\"metricKey\":\"accuracy\"},{\"uuid\":\"1716547047359xgfy895a\",\"type\":\"BAR\",\"runsCountToCompare\":10,\"metricSectionId\":\"1716542414630z4tttoib\",\"deleted\":false,\"isGenerated\":true,\"metricKey\":\"best_accuracy\"},{\"uuid\":\"17165424146300rhcc3uu\",\"type\":\"BAR\",\"runsCountToCompare\":10,\"metricSectionId\":\"1716542414630z4tttoib\",\"deleted\":false,\"isGenerated\":true,\"metricKey\":\"f1_score\"},{\"uuid\":\"1716542414630gla9iwad\",\"type\":\"BAR\",\"runsCountToCompare\":10,\"metricSectionId\":\"1716542414630z4tttoib\",\"deleted\":false,\"isGenerated\":true,\"metricKey\":\"loss\"},{\"uuid\":\"1716542414630b5pu6v6p\",\"type\":\"BAR\",\"runsCountToCompare\":10,\"metricSectionId\":\"1716542414630z4tttoib\",\"deleted\":false,\"isGenerated\":true,\"metricKey\":\"precision\"},{\"uuid\":\"17165424146306v3e4s01\",\"type\":\"BAR\",\"runsCountToCompare\":10,\"metricSectionId\":\"1716542414630z4tttoib\",\"deleted\":false,\"isGenerated\":true,\"metricKey\":\"recall\"},{\"uuid\":\"1716547242947j2l6u06c\",\"type\":\"PARALLEL\",\"runsCountToCompare\":10,\"metricSectionId\":\"1716542414630z4tttoib\",\"deleted\":false,\"isGenerated\":false,\"selectedParams\":[\"batch_size\",\"epochs\",\"epsilon\",\"learning_rate\",\"maxlen\"],\"selectedMetrics\":[\"accuracy\"],\"showAllRuns\":false}],\"compareRunSections\":[{\"uuid\":\"1716542414630z4tttoib\",\"name\":\"Model '\n",
       "                                                                                            'metrics\",\"display\":true,\"isReordered\":false,\"deleted\":false,\"isGenerated\":true},{\"uuid\":\"1716542414630dsxik0hj\",\"name\":\"System '\n",
       "                                                                                            'metrics\",\"display\":true,\"isReordered\":false,\"deleted\":false,\"isGenerated\":true}],\"viewMaximized\":false,\"runListHidden\":false,\"isAccordionReordered\":false,\"groupBy\":null,\"groupsExpanded\":{},\"autoRefreshEnabled\":false,\"runsExpanded\":{\"316d07fe1b984088b834158a9a3c3d85\":true,\"df7c1c1401e24ee383557ce39a1ab6d4\":true,\"74d564e912ca437fa2000a3c20515e16\":true,\"fba9268294724ebeb984a4c1a20aa9a7\":true,\"9ab0a5509b604145a1083cdb1d1073c4\":true,\"3840883535d94c3a97c53353b5f326a0\":true,\"960b06ec2dc7404eb2f294c4ac97e8b5\":true,\"909fdcb0074e4f2080f03e15a8694c02\":true,\"662fe2c31ec746e4b8fc3e1e9a589906\":true,\"5cfdae5e6da649e097fc4d6026c13ced\":true}}'}>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mlflow\n",
    "import optuna\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "def get_or_create_experiment(experiment_name):\n",
    "    \"\"\"\n",
    "    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "    This function checks if an experiment with the given name exists within MLflow.\n",
    "    If it does, the function returns its ID. If not, it creates a new experiment\n",
    "    with the provided name and returns its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "    - str: ID of the existing or newly created MLflow experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)\n",
    "experiment_id = get_or_create_experiment(\"FAKE_CNN\")\n",
    "mlflow.set_experiment(experiment_id=experiment_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53594c2c-38c8-4cef-b168-b761af6ec5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "081598bd-96f9-4aeb-8388-96e079a77efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([pd.read_csv(\"fake_train_balanced_indexed.csv\")]).reset_index()\n",
    "# pd.read_csv(\"fake_train_inc_full_backtranslated_aug_indexed_balanced_v1.csv\"), pd.read_csv(\"fake_train_full_paraphrased_aug_indexed_balanced_llama3.csv\")\n",
    "validate = pd.read_csv(\"fake_validate_balanced_indexed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcf60a47-6fed-4d03-acff-d9a156393dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "train['tokens']=train['text'].apply(lambda x: word_tokenize(x, language='portuguese'))\n",
    "validate['tokens']=validate['text'].apply(lambda x: word_tokenize(x, language='portuguese'))\n",
    "all_training_words = set([word for tokens in list(train[\"tokens\"].values) for word in tokens])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3cb6315-3e0b-47ae-ab09-ac250dfe538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with open(\"skip_s300.txt\", \"r\") as f: #skip_s300.txt cbow_s300.txt\n",
    "    for line in f:\n",
    "        token_emb = line.rstrip().split(\" \")\n",
    "        embedding = token_emb[-300:]\n",
    "        token = \" \".join(token_emb[:-300])\n",
    "        if token in all_training_words:\n",
    "            token_dict[token] = embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c43e60de-6791-4cc5-8457-305ad3826c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207467"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "T_VOCAB = sorted(list(all_training_words))\n",
    "tokenizer = Tokenizer(num_words=len(T_VOCAB), char_level=False)\n",
    "tokenizer.fit_on_texts(list(train[\"tokens\"].values))\n",
    "train_word_index = tokenizer.word_index\n",
    "len(train_word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab228caa-0af9-427d-b7e0-61ca41a2d3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embedding_weights = np.zeros((len(train_word_index)+1, 300))\n",
    "for word, i in train_word_index.items():\n",
    "    embedding_vector = token_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        train_embedding_weights[int(i)] = [float(embedding) for embedding in embedding_vector]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3471cb1a-2a6c-43b9-8868-66e576b8aac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data \n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/PY3/portuguese.pickle')\n",
    "\n",
    "train['sentence_token']=train['text'].apply(lambda x: sent_tokenizer.tokenize(x))\n",
    "train['train_seq']=train['sentence_token'].apply(lambda x:tokenizer.texts_to_sequences(x))\n",
    "\n",
    "validate['sentence_token']=validate['text'].apply(lambda x: sent_tokenizer.tokenize(x))\n",
    "validate['train_seq']=validate['sentence_token'].apply(lambda x:tokenizer.texts_to_sequences(x))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c81751f0-0c13-4cf1-a2d3-3705f620e43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "train['train_token']= train['train_seq'].apply(lambda x: list(itertools.chain.from_iterable(x)))\n",
    "validate['train_token']=validate['train_seq'].apply(lambda x: list(itertools.chain.from_iterable(x)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "acd62abb-5517-41ef-b1f8-644ec159d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_sizes = [1,2,3,4]\n",
    "num_filters = 32\n",
    "embed_size=300\n",
    "embedding_matrix=train_embedding_weights\n",
    "max_features=len(train_word_index)+1\n",
    "#maxlen=6*500#n*m\n",
    "import tensorflow\n",
    "from tensorflow.keras.metrics import Precision, Recall, F1Score\n",
    "def get_model(learning_rate, epsilon, maxlen, n, m):    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "    x = SpatialDropout1D(0.4)(x)\n",
    "    x = Reshape((n, m, 300))(x) #Reshape((n, m, 300))(x)\n",
    "    #print(x)\n",
    "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 2),activation='relu')(x)\n",
    "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 3),activation='relu')(x)\n",
    "    conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 4),activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    conv_4 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 1),activation='relu')(x)\n",
    "    conv_5 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 2),activation='relu')(x)\n",
    "    conv_6 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 3),activation='relu')(x)\n",
    "    \n",
    "    \n",
    "    maxpool_0 = MaxPool2D()(conv_0)\n",
    "    maxpool_0=Flatten()(maxpool_0)\n",
    "    maxpool_1 = MaxPool2D()(conv_1)\n",
    "    maxpool_1=Flatten()(maxpool_1)\n",
    "    maxpool_2 = MaxPool2D()(conv_2)\n",
    "    maxpool_2 = Flatten()(maxpool_2)\n",
    "    \n",
    "    maxpool_4 = MaxPool2D()(conv_4)\n",
    "    maxpool_4=Flatten()(maxpool_4)\n",
    "    maxpool_5 = MaxPool2D()(conv_5)\n",
    "    maxpool_5=Flatten()(maxpool_5)\n",
    "    maxpool_6 = MaxPool2D()(conv_6)\n",
    "    maxpool_6=Flatten()(maxpool_6)\n",
    "    #maxpool_7 = MaxPool2D()(conv_7)\n",
    "   # maxpool_7=Flatten()(maxpool_7)\n",
    "    z = concatenate([maxpool_0, maxpool_1,maxpool_2],axis=1)\n",
    "    w=concatenate([maxpool_4, maxpool_5,maxpool_6],axis=1)    \n",
    "    #w=concatenate([maxpool_4, maxpool_5,maxpool_6],axis=1)    \n",
    "    #z = concatenate([maxpool_0, maxpool_1,maxpool_2,maxpool_4, maxpool_5,maxpool_6],axis=1)\n",
    "    #z = concatenate([maxpool_0, maxpool_1,maxpool_4, maxpool_5],axis=1)\n",
    "    \n",
    "    #z = Flatten()(z)\n",
    "    z=concatenate([w,z],axis=1)\n",
    "    z=Dense(units=64,activation=\"relu\")(z)\n",
    "    z = Dropout(0.4)(z)\n",
    "        \n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    optimizer = tensorflow.keras.optimizers.Adam(learning_rate=learning_rate, epsilon = epsilon)\n",
    "    model.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy', Precision(), Recall(), F1Score()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "32f19058-ed41-4758-a78e-1b84f6f34d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import gc\n",
    "from numba import cuda \n",
    "import sys\n",
    "from multiprocessing import Queue, Process\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from typing import Optional\n",
    "from typing import Any\n",
    "from typing import Dict\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "class HEnconde(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        json.JSONEncoder.default(self, obj)\n",
    "class TFKerasPruningCallback(TFKerasPruningCallback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        # Allow instances to be re-used\n",
    "        self.best_weights = None\n",
    "        self.best_acc = 0\n",
    "    \n",
    "    def on_epoch_end(self, epoch: int, logs: Optional[Dict[str, Any]] = None) -> None:\n",
    "        logs = logs or {}\n",
    "        current_score = logs.get(self._monitor)\n",
    "        if float(self.best_acc) < float(current_score):\n",
    "            self.best_acc = current_score\n",
    "            self.best_weights = self.model.get_weights()\n",
    "        if current_score is None:\n",
    "            message = (\n",
    "                \"The metric '{}' is not in the evaluation logs for pruning. \"\n",
    "                \"Please make sure you set the correct metric name.\".format(self._monitor)\n",
    "            )\n",
    "            warnings.warn(message)\n",
    "            return\n",
    "\n",
    "        # Report current score and epoch to Optuna's trial.\n",
    "        self._trial.report(float(current_score), step=epoch)\n",
    "\n",
    "        # Prune trial if needed\n",
    "        if self._trial.should_prune():\n",
    "            message = \"Trial was pruned at epoch {}.\".format(epoch)\n",
    "            self.model.stop_training = True\n",
    "            #raise optuna.TrialPruned(message)\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.best_weights is not None:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "    \n",
    "def train_model(params, callbacks):\n",
    "    def pad_split(x):\n",
    "            x += ((params[\"maxlen\"] - len(x)) if len(x)<params[\"maxlen\"] else 0) * [0]    \n",
    "            return x[:params[\"maxlen\"]]\n",
    "    train_cnn = train['train_token'].apply(lambda x: pad_split(x))\n",
    "    validate_cnn = validate['train_token'].apply(lambda x: pad_split(x))\n",
    "    tf.keras.utils.set_random_seed(5113)\n",
    "    model = get_model(params[\"learning_rate\"], params[\"epsilon\"], params[\"maxlen\"], 6, int(params[\"maxlen\"]/6))\n",
    "    history=model.fit(list(train_cnn.values), [float(x) for x in train[\"misinformation\"].values],validation_data=(list(validate_cnn.values), [float(x) for x in validate[\"misinformation\"].values]), epochs=params[\"epochs\"],batch_size=params[\"batch_size\"],verbose=0,callbacks = callbacks,)\n",
    "    #metrics=model.evaluate(list(validate_cnn.values), [float(x) for x in validate[\"misinformation\"].values], verbose=0)\n",
    "    best_acc_i = history.history['val_accuracy'].index(max(history.history['val_accuracy']))\n",
    "    metrics = []\n",
    "    for metric_name in model.metrics_names:\n",
    "        metrics.append(history.history[\"val_\" + metric_name][best_acc_i])\n",
    "    for best_m in glob.glob(\"best_cnn_*.h5\"):\n",
    "        if float(best_m[:-3].replace(\"best_cnn_\", \"\")) < float(history.history['val_accuracy'][best_acc_i]):\n",
    "            model.save_weights(f\"best_cnn_{str(history.history['val_accuracy'][best_acc_i])}.h5\")\n",
    "            os.remove(best_m)\n",
    "            with open(f\"best_cnn_history.json\", \"w\") as f:\n",
    "                f.write(json.dumps(dict(history.history), indent=4, cls=HEnconde))\n",
    "    return [best_acc_i, zip(model.metrics_names, metrics)]\n",
    "\n",
    "#https://stackoverflow.com/questions/39758094/clearing-tensorflow-gpu-memory-after-model-execution\n",
    "def process_run(func, params, callbacks):\n",
    "    def wrapper_func(queue, params, callbacks):\n",
    "        try:\n",
    "            result = func(params, callbacks)\n",
    "            error = None\n",
    "        except Exception:\n",
    "            result = None\n",
    "            ex_type, ex_value, tb = sys.exc_info()\n",
    "            error = ex_type, ex_value\n",
    "        queue.put((result, error))\n",
    "\n",
    "    def process(params, callbacks):\n",
    "        queue = Queue()\n",
    "        p = Process(target = wrapper_func, args = [queue] + [params] + [callbacks])\n",
    "        p.start()\n",
    "        result, error = queue.get()\n",
    "        p.join()\n",
    "        return result, error  \n",
    "\n",
    "    result, error = process(params, callbacks)\n",
    "    return result, error\n",
    "\n",
    "\n",
    "def objective(trial, study):\n",
    "    # Define hyperparameters\n",
    "    params = {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-3),\n",
    "        \"epsilon\": trial.suggest_float(\"epsilon\", 1e-8, 1e-5),\n",
    "        \"maxlen\": trial.suggest_int(\"maxlen\", 600, 3000, step=600),\n",
    "        \"epochs\": 20, #trial.suggest_int(\"epochs\", 2, 12, step=2)\n",
    "        \"batch_size\":trial.suggest_int(\"batch_size\", 8, 64, step=8)\n",
    "    }\n",
    "    callbacks = [TFKerasPruningCallback(trial, \"val_accuracy\")]\n",
    "    #TFKerasPruningCallback(trial, \"val_accuracy\")\n",
    "    #tensorflow.keras.callbacks.EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=3, verbose=0, mode=\"max\"), \n",
    "    with mlflow.start_run(nested=True, run_name=f\"CNN_MODEL_NO_AUG_SKIP_lr_{params['learning_rate']}_eps_{params['epsilon']}_maxlen_{params['maxlen']}_epochs_{params['epochs']}_bs_{params['batch_size']}\"):\n",
    "        metrics, error = process_run(train_model, params=params, callbacks=callbacks)\n",
    "        #metrics = train_model(params, callbacks)\n",
    "        # Log to MLflow\n",
    "        print(error)\n",
    "        params[\"epochs\"] = metrics[0] + 1\n",
    "        mlflow.log_params(params)\n",
    "        metrics = list(metrics[1])\n",
    "        metrics = dict(metrics)\n",
    "        metrics[\"f1_score\"] =  2 * (metrics[\"precision\"] * metrics[\"recall\"]) / (metrics[\"precision\"] + metrics[\"recall\"])\n",
    "        for metric in metrics:\n",
    "            mlflow.log_metric(metric, metrics[metric])\n",
    "        print(dict(metrics))\n",
    "    return metrics[\"accuracy\"]\n",
    "        #best_acc = []\n",
    "        #for i in range(params[\"epochs\"]):\n",
    "        #    best_acc.append(i, trial.intermediate_values[i])\n",
    "        #best_acc = max(best_acc, key = lambda x: x[1])\n",
    "        #params[\"epochs\"] = best_acc[0] + 1\n",
    "        #mlflow.log_params(params)\n",
    "        #if not metrics and best_acc <= best_value:\n",
    "        #    print(params, error)\n",
    "        #    raise optuna.TrialPruned()\n",
    "        #mlflow.log_metric(\"accuracy\", best_acc)\n",
    "    #return best_acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3a02639-e2d0-4abf-a1f1-bfbf7fc967d9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/markt/tcc/jupyter/jupyter-ml/lib/python3.10/site-packages/optuna/samplers/_tpe/sampler.py:319: ExperimentalWarning: ``multivariate`` option is an experimental feature. The interface can change in the future.\n",
      "  warnings.warn(\n",
      "[I 2024-06-08 13:21:16,860] A new study created in RDB with name: CNN_MODEL_NO_AUG_SKIP_V10\n",
      "2024-06-08 13:21:18.824874: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:21:18.921469: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:21:18.921580: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:21:18.925066: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:21:18.925204: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:21:18.925250: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:21:19.177252: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:21:19.177408: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:21:19.177437: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 13:21:19.177501: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:21:19.177531: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 13:22:41.185190: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 13:22:43.817070: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3400f4c10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 13:22:43.817110: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 13:22:43.823323: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717863763.895903    6592 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 13:31:55,246] Trial 0 finished with value: 0.8099762201309204 and parameters: {'learning_rate': 0.0005192129639433677, 'epsilon': 2.3115021918505433e-06, 'maxlen': 2400, 'batch_size': 48}. Best is trial 0 with value: 0.8099762201309204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.6402490139007568, 'accuracy': 0.8099762201309204, 'precision': 0.7794432640075684, 'recall': 0.8646080493927002, 'f1_score': 0.8198198129376624}\n",
      "Initial trial 0 achieved value: 0.8099762201309204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 13:31:56.385230: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:31:56.460489: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:31:56.460604: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:31:56.462759: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:31:56.462860: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:31:56.462905: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:31:56.641641: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:31:56.641816: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:31:56.641843: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 13:31:56.641906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:31:56.641938: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 13:33:36.079024: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 323520000 exceeds 10% of free system memory.\n",
      "2024-06-08 13:33:36.326181: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 323520000 exceeds 10% of free system memory.\n",
      "2024-06-08 13:33:38.341172: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 13:33:39.877557: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd25c6a61a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 13:33:39.877595: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 13:33:39.882925: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717864419.955892   12158 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 13:56:03,727] Trial 1 finished with value: 0.7980997562408447 and parameters: {'learning_rate': 0.0006669337711146392, 'epsilon': 5.7582039501047775e-06, 'maxlen': 3000, 'batch_size': 8}. Best is trial 0 with value: 0.8099762201309204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 1.632920265197754, 'accuracy': 0.7980997562408447, 'precision': 0.7839366793632507, 'recall': 0.8230403661727905, 'f1_score': 0.8030127542041988}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 13:56:04.590565: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:56:04.669560: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:56:04.669669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:56:04.672862: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:56:04.672998: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:56:04.673073: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:56:04.837175: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:56:04.837332: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:56:04.837354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 13:56:04.837413: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 13:56:04.837444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 13:57:07.193696: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 13:57:09.072075: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd338a3ede0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 13:57:09.072116: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 13:57:09.077906: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717865829.146793   29518 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 14:05:16,961] Trial 2 finished with value: 0.8040379881858826 and parameters: {'learning_rate': 0.0007634351007094738, 'epsilon': 3.2379204319983965e-06, 'maxlen': 1800, 'batch_size': 32}. Best is trial 0 with value: 0.8099762201309204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.6741845607757568, 'accuracy': 0.8040379881858826, 'precision': 0.772921085357666, 'recall': 0.8610451221466064, 'f1_score': 0.8146067247841036}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 14:05:17.984732: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:05:18.062237: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:05:18.062339: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:05:18.064127: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:05:18.064288: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:05:18.064382: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:05:18.232945: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:05:18.233106: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:05:18.233125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 14:05:18.233194: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:05:18.233235: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 14:06:40.162177: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 14:06:41.647559: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd26361b5c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 14:06:41.647597: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 14:06:41.653173: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717866401.721603    4089 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 14:27:43,659] Trial 3 finished with value: 0.8087885975837708 and parameters: {'learning_rate': 0.00018510806389731478, 'epsilon': 9.406114381453813e-06, 'maxlen': 2400, 'batch_size': 8}. Best is trial 0 with value: 0.8099762201309204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.652758002281189, 'accuracy': 0.8087885975837708, 'precision': 0.7653061151504517, 'recall': 0.8907363414764404, 'f1_score': 0.8232711261608013}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 14:27:44.386063: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:27:44.443496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:27:44.443598: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:27:44.445443: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:27:44.445556: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:27:44.445602: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:27:44.610416: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:27:44.610547: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:27:44.610566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 14:27:44.610629: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:27:44.610654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 14:28:28.749678: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 14:28:30.373946: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd25f0d98c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 14:28:30.373989: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 14:28:30.379385: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717867710.447563   20710 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-06-08 14:28:30.701505: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 14:28:30.701566: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 14:28:30.727437: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 14:28:30.727498: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 14:28:30.770130: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 14:28:30.770193: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 14:28:30.795880: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 14:28:30.795935: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 4.74GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 14:28:30.836474: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 14:28:30.836535: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.12GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "[I 2024-06-08 14:31:59,688] Trial 4 finished with value: 0.7963182926177979 and parameters: {'learning_rate': 0.0009299013797761582, 'epsilon': 9.123282705171441e-06, 'maxlen': 1200, 'batch_size': 24}. Best is trial 0 with value: 0.8099762201309204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.6270310282707214, 'accuracy': 0.7963182926177979, 'precision': 0.7497497200965881, 'recall': 0.8895487189292908, 'f1_score': 0.813688206066773}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 14:32:00.299289: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:32:00.380945: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:32:00.381085: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:32:00.383283: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:32:00.383391: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:32:00.383437: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:32:00.568526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:32:00.568740: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:32:00.568783: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 14:32:00.568864: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:32:00.568913: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 14:32:24.261547: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 14:32:25.886633: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd334170540 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 14:32:25.886675: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 14:32:25.893021: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717867945.968844   27867 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 14:34:48,558] Trial 5 finished with value: 0.6306413412094116 and parameters: {'learning_rate': 2.3613975141778516e-05, 'epsilon': 8.876702362229401e-06, 'maxlen': 600, 'batch_size': 16}. Best is trial 0 with value: 0.8099762201309204.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.6341243982315063, 'accuracy': 0.6306413412094116, 'precision': 0.5924369692802429, 'recall': 0.8372921347618103, 'f1_score': 0.6938976248270894}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 14:34:49.452270: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:34:49.529371: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:34:49.529475: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:34:49.531578: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:34:49.531686: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:34:49.531743: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:34:49.702056: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:34:49.702183: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:34:49.702205: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 14:34:49.702264: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:34:49.702296: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 14:35:54.434304: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 14:35:56.940869: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd26b6c52d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 14:35:56.940910: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 14:35:56.946540: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717868157.022914    3733 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 14:42:43,068] Trial 6 finished with value: 0.8117577433586121 and parameters: {'learning_rate': 0.00023834306926025657, 'epsilon': 8.938436169241089e-06, 'maxlen': 1800, 'batch_size': 64}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.6623251438140869, 'accuracy': 0.8117577433586121, 'precision': 0.7777777910232544, 'recall': 0.8729215860366821, 'f1_score': 0.8226077168980166}\n",
      "Trial 6 achieved value: 0.8117577433586121 with  0.2195% improvement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 14:42:44.383064: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:42:44.461076: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:42:44.461236: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:42:44.463189: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:42:44.463321: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:42:44.463369: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:42:44.651241: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:42:44.651370: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:42:44.651393: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 14:42:44.651458: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:42:44.651482: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 14:44:33.494320: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 14:44:35.746002: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd33c082870 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 14:44:35.746047: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 14:44:35.758222: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717868675.891035    8469 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 14:57:10,029] Trial 7 finished with value: 0.8070071339607239 and parameters: {'learning_rate': 0.0006465847999863442, 'epsilon': 1.757627705295045e-06, 'maxlen': 3000, 'batch_size': 24}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.7079646587371826, 'accuracy': 0.8070071339607239, 'precision': 0.7673215866088867, 'recall': 0.8812351822853088, 'f1_score': 0.8203427276578459}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 14:57:11.114350: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:57:11.178980: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:57:11.179089: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:57:11.180921: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:57:11.181048: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:57:11.181148: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:57:11.346552: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:57:11.346683: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:57:11.346702: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 14:57:11.346782: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 14:57:11.346817: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 14:58:39.623817: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 14:58:41.236770: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd25efeb5c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 14:58:41.236811: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 14:58:41.244703: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717869521.320349   17380 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:07:42,626] Trial 8 finished with value: 0.8058194518089294 and parameters: {'learning_rate': 0.0002941849937435305, 'epsilon': 2.079552084511895e-06, 'maxlen': 2400, 'batch_size': 8}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.4353448152542114, 'accuracy': 0.8058194518089294, 'precision': 0.7624872326850891, 'recall': 0.8883610367774963, 'f1_score': 0.82062532467413}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:07:43.925975: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:07:44.015329: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:07:44.015427: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:07:44.017098: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:07:44.017217: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:07:44.017295: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:07:44.214286: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:07:44.214432: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:07:44.214454: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:07:44.214561: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:07:44.214621: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 15:09:34.204828: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 15:09:35.766270: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd25eeec1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 15:09:35.766313: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 15:09:35.771617: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717870175.847640   32354 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:15:00,177] Trial 9 finished with value: 0.783847987651825 and parameters: {'learning_rate': 0.00018457104622165293, 'epsilon': 6.782763431415885e-06, 'maxlen': 3000, 'batch_size': 8}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.44580987095832825, 'accuracy': 0.783847987651825, 'precision': 0.7394789457321167, 'recall': 0.8764845728874207, 'f1_score': 0.8021739110335114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:15:01.144547: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:15:01.203310: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:15:01.203423: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:15:01.205038: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:15:01.205142: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:15:01.205189: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:15:01.375735: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:15:01.375881: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:15:01.375905: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:15:01.375967: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:15:01.376000: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 15:16:03.437239: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 15:16:06.053901: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd33404ae70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 15:16:06.053941: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 15:16:06.060079: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717870566.128566   15246 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:18:44,359] Trial 10 finished with value: 0.7363420724868774 and parameters: {'learning_rate': 8.448049257013131e-05, 'epsilon': 8.615501599136706e-06, 'maxlen': 1800, 'batch_size': 64}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5500140190124512, 'accuracy': 0.7363420724868774, 'precision': 0.6721453070640564, 'recall': 0.9228028655052185, 'f1_score': 0.7777777686599481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:18:45.454446: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:18:45.512471: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:18:45.512589: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:18:45.514845: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:18:45.514976: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:18:45.515101: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:18:45.693113: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:18:45.693244: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:18:45.693263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:18:45.693332: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:18:45.693361: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 15:20:08.247340: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 15:20:11.193075: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd330a0c010 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 15:20:11.193145: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 15:20:11.199517: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717870811.269856   18895 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:21:50,663] Trial 11 finished with value: 0.7553443908691406 and parameters: {'learning_rate': 0.0004134024705307368, 'epsilon': 2.2129863953591866e-06, 'maxlen': 2400, 'batch_size': 64}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5108011364936829, 'accuracy': 0.7553443908691406, 'precision': 0.692307710647583, 'recall': 0.91923987865448, 'f1_score': 0.7897959205819779}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:21:51.661856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:21:51.735834: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:21:51.735932: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:21:51.738020: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:21:51.738136: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:21:51.738183: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:21:51.939990: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:21:51.940124: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:21:51.940148: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:21:51.940205: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:21:51.940237: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 15:22:53.891583: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 15:22:56.167941: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd330aabda0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 15:22:56.167983: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 15:22:56.174029: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717870976.249245   22313 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:24:28,810] Trial 12 finished with value: 0.7701900005340576 and parameters: {'learning_rate': 0.0003849187422816467, 'epsilon': 6.8383523277109164e-06, 'maxlen': 1800, 'batch_size': 48}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.4695214629173279, 'accuracy': 0.7701900005340576, 'precision': 0.7124183177947998, 'recall': 0.9061757922172546, 'f1_score': 0.7976999663158031}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:24:29.998198: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:24:30.057591: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:24:30.057700: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:24:30.059561: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:24:30.059691: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:24:30.059762: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:24:30.241077: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:24:30.241206: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:24:30.241243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:24:30.241363: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:24:30.241394: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 15:26:11.722023: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 15:26:14.969085: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3380dfca0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 15:26:14.969126: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 15:26:14.975220: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717871175.044875   26445 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:36:00,219] Trial 13 finished with value: 0.8087885975837708 and parameters: {'learning_rate': 0.0003859638650497791, 'epsilon': 9.425839461265936e-06, 'maxlen': 3000, 'batch_size': 56}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.6795080900192261, 'accuracy': 0.8087885975837708, 'precision': 0.7691511511802673, 'recall': 0.8824228048324585, 'f1_score': 0.8219026628431239}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:36:01.137660: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:36:01.214403: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:36:01.214523: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:36:01.216535: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:36:01.216694: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:36:01.216773: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:36:01.395865: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:36:01.396019: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:36:01.396042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:36:01.396103: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:36:01.396134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 15:37:22.583420: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 15:37:25.249551: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd334171cc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 15:37:25.249596: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 15:37:25.255759: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717871845.333121   31905 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:43:53,575] Trial 14 finished with value: 0.8105700612068176 and parameters: {'learning_rate': 0.0008069877946473216, 'epsilon': 9.943631545371735e-06, 'maxlen': 1800, 'batch_size': 64}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.8793688416481018, 'accuracy': 0.8105700612068176, 'precision': 0.788313090801239, 'recall': 0.8491686582565308, 'f1_score': 0.8176100527373026}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:43:54.505183: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:43:54.592726: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:43:54.592867: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:43:54.594980: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:43:54.595245: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:43:54.595348: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:43:54.799275: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:43:54.799448: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:43:54.799474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:43:54.799540: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:43:54.799575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 15:45:03.405215: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 15:45:05.864346: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd330012fc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 15:45:05.864388: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 15:45:05.870571: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717872305.946695    4002 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:46:35,976] Trial 15 finished with value: 0.7731591463088989 and parameters: {'learning_rate': 0.0007887084835665412, 'epsilon': 9.869016286973431e-06, 'maxlen': 1800, 'batch_size': 56}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.48973312973976135, 'accuracy': 0.7731591463088989, 'precision': 0.7102376818656921, 'recall': 0.9228028655052185, 'f1_score': 0.8026859701317411}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:46:36.585409: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:46:36.657219: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:46:36.657346: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:46:36.659219: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:46:36.659422: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:46:36.659524: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:46:36.852501: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:46:36.852692: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:46:36.852717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:46:36.852797: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:46:36.852829: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 15:47:00.686966: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 15:47:02.943385: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd32f528f20 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 15:47:02.943426: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 15:47:02.949195: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717872423.020330    7919 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:48:13,370] Trial 16 finished with value: 0.7749406099319458 and parameters: {'learning_rate': 0.0006038216330077227, 'epsilon': 9.41789437455089e-06, 'maxlen': 600, 'batch_size': 48}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.48208898305892944, 'accuracy': 0.7749406099319458, 'precision': 0.7110300660133362, 'recall': 0.9263657927513123, 'f1_score': 0.8045384104848121}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:48:14.632939: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:48:14.709823: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:48:14.709927: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:48:14.711924: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:48:14.712070: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:48:14.712118: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:48:14.908962: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:48:14.909085: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:48:14.909110: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:48:14.909169: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:48:14.909200: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 15:50:06.009467: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 15:50:09.572886: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3240d8f10 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 15:50:09.572929: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 15:50:09.579571: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717872609.652298   11717 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 15:59:45,594] Trial 17 finished with value: 0.7992874383926392 and parameters: {'learning_rate': 0.0009532116500169403, 'epsilon': 8.711523679669719e-06, 'maxlen': 3000, 'batch_size': 64}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.44517165422439575, 'accuracy': 0.7992874383926392, 'precision': 0.7658227682113647, 'recall': 0.8622328042984009, 'f1_score': 0.8111731861984564}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 15:59:46.218620: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:59:46.304373: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:59:46.304485: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:59:46.306327: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:59:46.306515: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:59:46.306593: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:59:46.511119: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:59:46.511268: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:59:46.511292: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 15:59:46.511352: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 15:59:46.511383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:00:11.691448: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:00:12.112412: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:12.187240: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:12.245216: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:12.290509: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:12.358388: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:12.418125: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:12.495739: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:12.568148: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:12.630461: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:12.687682: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:00:13.463874: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd334172a40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:00:13.463929: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:00:13.472902: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717873213.549037   16467 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 16:01:53,596] Trial 18 finished with value: 0.7927553653717041 and parameters: {'learning_rate': 0.0002975300708347046, 'epsilon': 2.7879009322913544e-06, 'maxlen': 600, 'batch_size': 64}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5024035573005676, 'accuracy': 0.7927553653717041, 'precision': 0.7363374829292297, 'recall': 0.9121140241622925, 'f1_score': 0.8148541122463537}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:01:54.627639: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:01:54.692150: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:01:54.692265: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:01:54.694763: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:01:54.694957: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:01:54.695037: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:01:54.933354: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:01:54.933506: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:01:54.933527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 16:01:54.933586: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:01:54.933617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:03:04.880851: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:03:07.538646: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd334190db0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:03:07.538690: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:03:07.545022: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717873387.617754   19851 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 16:04:34,233] Trial 19 finished with value: 0.7624703049659729 and parameters: {'learning_rate': 0.0007942486055669815, 'epsilon': 4.487005097634439e-06, 'maxlen': 1800, 'batch_size': 64}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5185354948043823, 'accuracy': 0.7624703049659729, 'precision': 0.7001811861991882, 'recall': 0.9180522561073303, 'f1_score': 0.7944501712408866}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:04:35.535821: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:04:35.612919: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:04:35.613033: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:04:35.614835: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:04:35.614976: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:04:35.615047: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:04:35.830631: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:04:35.830856: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:04:35.830888: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 16:04:35.830970: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:04:35.831013: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:06:29.112895: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:06:32.718870: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3341b22a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:06:32.718919: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:06:32.727689: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717873592.805079   23226 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 16:10:24,679] Trial 20 finished with value: 0.79038006067276 and parameters: {'learning_rate': 0.00019531517120717997, 'epsilon': 6.587276913915437e-06, 'maxlen': 3000, 'batch_size': 64}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.46847259998321533, 'accuracy': 0.79038006067276, 'precision': 0.7413622736930847, 'recall': 0.8919239640235901, 'f1_score': 0.8097034832722108}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:10:25.982908: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:10:26.067749: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:10:26.067867: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:10:26.069845: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:10:26.070034: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:10:26.070111: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:10:26.336685: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:10:26.336819: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:10:26.336841: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 16:10:26.336901: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:10:26.336930: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:12:18.847005: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:12:21.491956: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd2634b9680 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:12:21.492005: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:12:21.499511: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717873941.573422   27045 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 16:23:12,251] Trial 21 finished with value: 0.8099762201309204 and parameters: {'learning_rate': 0.000605041141290656, 'epsilon': 5.000092661028797e-06, 'maxlen': 3000, 'batch_size': 40}. Best is trial 6 with value: 0.8117577433586121.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5479399561882019, 'accuracy': 0.8099762201309204, 'precision': 0.791946291923523, 'recall': 0.8408551216125488, 'f1_score': 0.815668200781172}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:23:13.543495: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:23:13.622844: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:23:13.622964: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:23:13.625627: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:23:13.625911: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:23:13.626017: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:23:13.832066: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:23:13.832227: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:23:13.832250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 16:23:13.832317: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:23:13.832353: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:25:07.041984: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:25:10.416273: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd327e6fa70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:25:10.416330: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:25:10.422059: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717874710.509130    1112 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 16:35:34,178] Trial 22 finished with value: 0.8147268295288086 and parameters: {'learning_rate': 0.0007767899173042692, 'epsilon': 5.817048314025034e-07, 'maxlen': 3000, 'batch_size': 56}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5042393207550049, 'accuracy': 0.8147268295288086, 'precision': 0.8110328912734985, 'recall': 0.8206650614738464, 'f1_score': 0.8158205462642235}\n",
      "Trial 22 achieved value: 0.8147268295288086 with  0.3644% improvement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:35:35.503479: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:35:35.593029: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:35:35.593124: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:35:35.594951: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:35:35.595059: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:35:35.595104: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:35:35.849535: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:35:35.849683: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:35:35.849708: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 16:35:35.849789: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:35:35.849845: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:37:34.505284: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:37:37.619412: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd370095250 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:37:37.619452: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:37:37.626561: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717875457.698290    6737 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 16:39:43,944] Trial 23 finished with value: 0.7773159146308899 and parameters: {'learning_rate': 0.0006124639339745613, 'epsilon': 5.470763415486527e-07, 'maxlen': 3000, 'batch_size': 48}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.49417153000831604, 'accuracy': 0.7773159146308899, 'precision': 0.714811384677887, 'recall': 0.9228028655052185, 'f1_score': 0.8055987470831454}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:39:45.233499: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:39:45.319417: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:39:45.319546: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:39:45.321782: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:39:45.321973: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:39:45.322077: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:39:45.497390: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:39:45.497550: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:39:45.497575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 16:39:45.497652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:39:45.497724: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:41:38.718641: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:41:42.298659: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd33415c820 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:41:42.298703: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:41:42.304981: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717875702.377213   10995 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 16:51:21,033] Trial 24 finished with value: 0.8040379881858826 and parameters: {'learning_rate': 0.0009408508826014669, 'epsilon': 4.755534699938732e-07, 'maxlen': 3000, 'batch_size': 64}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5814569592475891, 'accuracy': 0.8040379881858826, 'precision': 0.7661122679710388, 'recall': 0.8752968907356262, 'f1_score': 0.8170731624744239}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:51:21.773040: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:51:21.849426: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:51:21.849532: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:51:21.851437: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:51:21.851574: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:51:21.851621: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:51:22.030824: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:51:22.030962: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:51:22.030981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 16:51:22.031046: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:51:22.031076: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:52:07.537077: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:52:09.716329: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd33814d3d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:52:09.716373: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:52:09.722107: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717876329.797485   15790 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-06-08 16:52:42.247852: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:52:42.247939: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:52:42.284949: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:52:42.285020: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:52:42.324176: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:52:42.324241: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:52:42.361350: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:52:42.361415: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:52:42.420346: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 16:52:42.420406: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "[I 2024-06-08 16:53:21,021] Trial 25 finished with value: 0.7583135366439819 and parameters: {'learning_rate': 0.0009449580441173193, 'epsilon': 9.668462700399564e-06, 'maxlen': 1200, 'batch_size': 64}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5289791822433472, 'accuracy': 0.7583135366439819, 'precision': 0.6909569501876831, 'recall': 0.9346793293952942, 'f1_score': 0.794548186397653}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:53:22.006570: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:53:22.094461: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:53:22.094596: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:53:22.096844: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:53:22.097011: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:53:22.097106: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:53:22.288785: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:53:22.288986: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:53:22.289015: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 16:53:22.289093: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:53:22.289135: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:54:30.866114: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:54:33.203912: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd263035a60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:54:33.203963: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:54:33.210282: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717876473.282428   19013 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 16:56:08,708] Trial 26 finished with value: 0.7779097557067871 and parameters: {'learning_rate': 0.0003193767728269945, 'epsilon': 9.73422562449191e-06, 'maxlen': 1800, 'batch_size': 48}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.4564042389392853, 'accuracy': 0.7779097557067871, 'precision': 0.729411780834198, 'recall': 0.8836104273796082, 'f1_score': 0.7991407088093807}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 16:56:09.943433: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:56:10.003914: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:56:10.004009: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:56:10.005795: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:56:10.005906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:56:10.005953: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:56:10.209406: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:56:10.209533: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:56:10.209561: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 16:56:10.209633: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 16:56:10.209666: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 16:58:01.731547: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 16:58:04.483280: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd253048940 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 16:58:04.483321: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 16:58:04.489318: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717876684.564232   23170 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 17:08:53,136] Trial 27 finished with value: 0.8064132928848267 and parameters: {'learning_rate': 0.0009740628876916961, 'epsilon': 1.851870799717405e-06, 'maxlen': 3000, 'batch_size': 40}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.7912984490394592, 'accuracy': 0.8064132928848267, 'precision': 0.780434787273407, 'recall': 0.8527315855026245, 'f1_score': 0.8149829737209769}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:08:54.097663: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:08:54.182733: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:08:54.182870: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:08:54.184956: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:08:54.185121: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:08:54.185226: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:08:54.387247: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:08:54.387414: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:08:54.387438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 17:08:54.387503: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:08:54.387533: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 17:10:02.693856: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 17:10:05.383532: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3381c4f90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 17:10:05.383573: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 17:10:05.389595: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717877405.465635   29563 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 17:16:34,946] Trial 28 finished with value: 0.8129453659057617 and parameters: {'learning_rate': 0.0005146166964882979, 'epsilon': 9.22044060881277e-06, 'maxlen': 1800, 'batch_size': 64}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.43673330545425415, 'accuracy': 0.8129453659057617, 'precision': 0.7830290198326111, 'recall': 0.8657957315444946, 'f1_score': 0.8223350389182634}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:16:36.221530: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:16:36.313041: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:16:36.313218: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:16:36.316047: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:16:36.316273: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:16:36.316367: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:16:36.509531: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:16:36.509758: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:16:36.509798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 17:16:36.509931: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:16:36.509984: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 17:18:31.712531: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 17:18:35.423071: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd32c8b5270 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 17:18:35.423112: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 17:18:35.428642: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717877915.510544    1744 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 17:20:30,287] Trial 29 finished with value: 0.754156768321991 and parameters: {'learning_rate': 0.000832153280378978, 'epsilon': 3.963262553130108e-06, 'maxlen': 3000, 'batch_size': 64}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5152807235717773, 'accuracy': 0.754156768321991, 'precision': 0.6880491971969604, 'recall': 0.929928719997406, 'f1_score': 0.7909090753897466}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:20:30.850178: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:20:30.913173: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:20:30.913320: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:20:30.915229: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:20:30.915417: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:20:30.915512: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:20:31.107373: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:20:31.107526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:20:31.107547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 17:20:31.107605: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:20:31.107638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 17:20:55.518973: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 17:20:55.972116: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:56.058293: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:56.129729: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:56.185949: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:56.234948: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:56.278169: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:56.347807: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:56.410120: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:56.461208: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:56.505234: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:20:57.277178: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd324190c40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 17:20:57.277221: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 17:20:57.284135: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717878057.364817    5357 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 17:24:31,941] Trial 30 finished with value: 0.7850356101989746 and parameters: {'learning_rate': 6.158669211884466e-05, 'epsilon': 8.411180655228095e-06, 'maxlen': 600, 'batch_size': 64}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.48801738023757935, 'accuracy': 0.7850356101989746, 'precision': 0.741448700428009, 'recall': 0.8752968907356262, 'f1_score': 0.8028322398671335}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:24:32.708870: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:24:32.769197: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:24:32.769322: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:24:32.785967: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:24:32.786130: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:24:32.786205: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:24:32.949927: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:24:32.950092: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:24:32.950114: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 17:24:32.950177: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:24:32.950209: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 17:25:18.516353: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 17:25:20.736370: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3300c74e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 17:25:20.736416: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 17:25:20.742060: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717878320.822670    9490 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-06-08 17:25:52.187690: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:25:52.187756: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:25:52.225509: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:25:52.225577: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:25:52.265065: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:25:52.265135: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:25:52.302441: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:25:52.302518: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:25:52.362869: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 17:25:52.362939: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "[I 2024-06-08 17:26:30,292] Trial 31 finished with value: 0.7577196955680847 and parameters: {'learning_rate': 0.0004943238618495547, 'epsilon': 9.055728174850775e-06, 'maxlen': 1200, 'batch_size': 64}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5040186047554016, 'accuracy': 0.7577196955680847, 'precision': 0.6937500238418579, 'recall': 0.9228028655052185, 'f1_score': 0.7920489507821223}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:26:31.398903: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:26:31.481927: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:26:31.482198: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:26:31.484555: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:26:31.484802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:26:31.484920: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:26:31.677215: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:26:31.677393: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:26:31.677418: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 17:26:31.677488: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:26:31.677517: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 17:27:59.727992: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 17:28:02.821277: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3340dd8a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 17:28:02.821320: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 17:28:02.828121: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717878482.903862   12652 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 17:36:21,025] Trial 32 finished with value: 0.8129453659057617 and parameters: {'learning_rate': 0.0005185724263896135, 'epsilon': 8.447291250312407e-06, 'maxlen': 2400, 'batch_size': 64}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5147228240966797, 'accuracy': 0.8129453659057617, 'precision': 0.7724922299385071, 'recall': 0.8871734142303467, 'f1_score': 0.825870646306195}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:36:22.173342: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:36:22.235652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:36:22.235785: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:36:22.237861: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:36:22.238059: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:36:22.238110: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:36:22.534122: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:36:22.534609: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:36:22.534676: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 17:36:22.534907: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:36:22.534978: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 17:37:52.093968: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 17:37:55.017764: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3340c6400 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 17:37:55.017820: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 17:37:55.023772: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717879075.100963   17296 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 17:39:41,415] Trial 33 finished with value: 0.771377682685852 and parameters: {'learning_rate': 0.00063268665391056, 'epsilon': 8.278021366056683e-06, 'maxlen': 2400, 'batch_size': 56}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.47670671343803406, 'accuracy': 0.771377682685852, 'precision': 0.7182425856590271, 'recall': 0.8931116461753845, 'f1_score': 0.7961884548513329}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:39:42.483640: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:39:42.551838: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:39:42.552122: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:39:42.554561: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:39:42.555043: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:39:42.555160: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:39:42.743364: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:39:42.743516: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:39:42.743536: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 17:39:42.743632: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:39:42.743671: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 17:41:11.211966: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 17:41:14.175903: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd334163f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 17:41:14.175946: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 17:41:14.181529: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717879274.253580   21105 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 17:49:56,860] Trial 34 finished with value: 0.8105700612068176 and parameters: {'learning_rate': 0.0004908449667265582, 'epsilon': 7.205849390330521e-06, 'maxlen': 2400, 'batch_size': 64}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5804195404052734, 'accuracy': 0.8105700612068176, 'precision': 0.7802786827087402, 'recall': 0.8646080493927002, 'f1_score': 0.8202816846575484}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:49:57.829397: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:49:57.913961: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:49:57.914091: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:49:57.916783: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:49:57.917134: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:49:57.917272: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:49:58.105983: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:49:58.106126: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:49:58.106147: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 17:49:58.106207: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:49:58.106233: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 17:51:04.491678: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 17:51:06.895694: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd338197a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 17:51:06.895736: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 17:51:06.902681: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717879866.976470   25890 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 17:58:09,339] Trial 35 finished with value: 0.8076009750366211 and parameters: {'learning_rate': 0.0005192185737705226, 'epsilon': 9.209586910197811e-06, 'maxlen': 1800, 'batch_size': 56}. Best is trial 22 with value: 0.8147268295288086.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.7808659672737122, 'accuracy': 0.8076009750366211, 'precision': 0.7778970003128052, 'recall': 0.8610451221466064, 'f1_score': 0.817361892739328}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 17:58:10.385309: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:58:10.447776: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:58:10.447871: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:58:10.449906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:58:10.450027: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:58:10.450077: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:58:10.627509: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:58:10.627671: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:58:10.627693: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 17:58:10.627767: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 17:58:10.627799: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 17:59:16.544552: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 17:59:19.163492: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd33816b9e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 17:59:19.163539: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 17:59:19.169346: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717880359.246527   30876 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:01:59,311] Trial 36 finished with value: 0.815914511680603 and parameters: {'learning_rate': 0.0007533635941153495, 'epsilon': 3.7273797381024064e-07, 'maxlen': 1800, 'batch_size': 64}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5391106009483337, 'accuracy': 0.815914511680603, 'precision': 0.7935982346534729, 'recall': 0.853919267654419, 'f1_score': 0.8226544754733525}\n",
      "Trial 36 achieved value: 0.815914511680603 with  0.1456% improvement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:02:00.259649: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:02:00.337526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:02:00.337632: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:02:00.339860: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:02:00.340125: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:02:00.340224: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:02:00.598073: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:02:00.598219: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:02:00.598242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:02:00.598304: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:02:00.598334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:03:06.654631: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:03:09.298499: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd338a5eb70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:03:09.298540: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:03:09.305197: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717880589.383493    2191 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:04:35,357] Trial 37 finished with value: 0.7654394507408142 and parameters: {'learning_rate': 0.0007020556192690625, 'epsilon': 5.034347851776995e-07, 'maxlen': 1800, 'batch_size': 64}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5263763070106506, 'accuracy': 0.7654394507408142, 'precision': 0.6969162821769714, 'recall': 0.9394299387931824, 'f1_score': 0.8002023189460783}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:04:35.986362: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:04:36.062667: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:04:36.062773: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:04:36.064624: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:04:36.064868: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:04:36.064981: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:04:36.273998: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:04:36.274141: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:04:36.274166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:04:36.274225: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:04:36.274257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:05:01.175119: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:05:03.045112: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd327577360 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:05:03.045158: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:05:03.050755: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717880703.127272    5718 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:08:05,382] Trial 38 finished with value: 0.8064132928848267 and parameters: {'learning_rate': 0.00042439886318406506, 'epsilon': 2.8361682870848014e-07, 'maxlen': 600, 'batch_size': 24}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.4261849820613861, 'accuracy': 0.8064132928848267, 'precision': 0.7632653117179871, 'recall': 0.8883610367774963, 'f1_score': 0.8210757406137907}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:08:06.261638: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:08:06.321973: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:08:06.322071: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:08:06.323983: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:08:06.324093: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:08:06.324141: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:08:06.494759: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:08:06.494974: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:08:06.495004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:08:06.495084: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:08:06.495119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:09:10.659888: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:09:13.253115: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3380cff30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:09:13.253159: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:09:13.259011: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717880953.327630   12365 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:11:50,999] Trial 39 finished with value: 0.8087885975837708 and parameters: {'learning_rate': 0.0009124011246119171, 'epsilon': 1.7721557334091364e-07, 'maxlen': 1800, 'batch_size': 64}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5322001576423645, 'accuracy': 0.8087885975837708, 'precision': 0.7844638824462891, 'recall': 0.8515439629554749, 'f1_score': 0.8166287039897474}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:11:52.130028: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:11:52.201933: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:11:52.202027: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:11:52.203965: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:11:52.204056: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:11:52.204100: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:11:52.377311: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:11:52.377436: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:11:52.377457: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:11:52.377513: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:11:52.377546: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:13:34.759665: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:13:38.178141: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd33c185d60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:13:38.178181: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:13:38.183709: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717881218.252388   16008 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:15:30,383] Trial 40 finished with value: 0.7808788418769836 and parameters: {'learning_rate': 0.0007238106707870167, 'epsilon': 7.097190672487575e-08, 'maxlen': 3000, 'batch_size': 64}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.44568854570388794, 'accuracy': 0.7808788418769836, 'precision': 0.7263157963752747, 'recall': 0.9014251828193665, 'f1_score': 0.8044515164275238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:15:31.265807: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:15:31.338568: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:15:31.338677: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:15:31.340563: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:15:31.340653: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:15:31.340698: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:15:31.504621: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:15:31.504818: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:15:31.504852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:15:31.504975: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:15:31.505011: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:16:35.996785: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:16:38.510689: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd33416ce60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:16:38.510731: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:16:38.516405: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717881398.584299   19466 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:18:01,973] Trial 41 finished with value: 0.745843231678009 and parameters: {'learning_rate': 0.0002647336761976575, 'epsilon': 9.403201249960806e-06, 'maxlen': 1800, 'batch_size': 64}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5099444389343262, 'accuracy': 0.745843231678009, 'precision': 0.6875, 'recall': 0.9014251828193665, 'f1_score': 0.7800616667031164}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:18:02.679367: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:18:02.754657: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:18:02.754756: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:18:02.756960: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:18:02.757081: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:18:02.757126: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:18:02.967963: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:18:02.968101: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:18:02.968126: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:18:02.968225: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:18:02.968260: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:18:46.987144: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:18:48.939129: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd330198b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:18:48.939173: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:18:48.945684: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717881529.018140   22826 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-06-08 18:18:49.297968: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:18:49.298033: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:18:49.339016: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:18:49.339073: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:18:49.396684: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.86GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:18:49.396745: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.86GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:18:49.437813: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.86GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:18:49.437873: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.86GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:18:49.502095: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:18:49.502160: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 7.10GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "[I 2024-06-08 18:24:20,246] Trial 42 finished with value: 0.8070071339607239 and parameters: {'learning_rate': 0.0006941210850087182, 'epsilon': 2.1894445233407234e-07, 'maxlen': 1200, 'batch_size': 48}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.599049985408783, 'accuracy': 0.8070071339607239, 'precision': 0.8002322912216187, 'recall': 0.8182897567749023, 'f1_score': 0.8091602925739924}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:24:21.436008: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:24:21.515095: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:24:21.515200: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:24:21.516995: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:24:21.517104: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:24:21.517151: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:24:21.707482: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:24:21.707643: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:24:21.707672: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:24:21.707742: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:24:21.707775: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:25:47.894329: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:25:50.927084: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3242d8880 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:25:50.927123: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:25:50.932834: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717881951.007198   28058 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:27:32,452] Trial 43 finished with value: 0.764845609664917 and parameters: {'learning_rate': 0.0005805498531971936, 'epsilon': 9.431948816370028e-06, 'maxlen': 2400, 'batch_size': 64}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.48842501640319824, 'accuracy': 0.764845609664917, 'precision': 0.70802241563797, 'recall': 0.9014251828193665, 'f1_score': 0.7931034673864653}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:27:33.072988: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:27:33.155937: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:27:33.156077: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:27:33.158862: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:27:33.159484: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:27:33.159719: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:27:33.454249: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:27:33.454378: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:27:33.454401: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:27:33.454461: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:27:33.454491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:27:57.398871: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:27:57.740957: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:57.816521: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:57.872331: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:57.917063: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:57.980416: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:58.038531: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:58.091164: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:58.138007: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:58.207408: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:58.268031: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:27:59.068430: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3340cfa70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:27:59.068492: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:27:59.074278: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717882079.148255   31607 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:29:37,084] Trial 44 finished with value: 0.8141329884529114 and parameters: {'learning_rate': 0.000992778167592403, 'epsilon': 2.6265808546689237e-06, 'maxlen': 600, 'batch_size': 64}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.4588354825973511, 'accuracy': 0.8141329884529114, 'precision': 0.7897042632102966, 'recall': 0.8562945127487183, 'f1_score': 0.8216524060137992}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:29:37.689315: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:29:37.766200: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:29:37.766304: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:29:37.768370: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:29:37.768474: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:29:37.768518: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:29:37.976206: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:29:37.976377: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:29:37.976405: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:29:37.976497: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:29:37.976527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:30:01.651788: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:30:02.054754: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:02.129764: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:02.186889: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:02.231738: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:02.300693: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:02.359973: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:02.409876: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:02.455636: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:02.533610: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:02.606281: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.59GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:30:03.372419: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd32b589510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:30:03.372478: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:30:03.378690: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717882203.450814    2531 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:31:41,605] Trial 45 finished with value: 0.8070071339607239 and parameters: {'learning_rate': 0.0009562150481890625, 'epsilon': 7.01963399893428e-07, 'maxlen': 600, 'batch_size': 64}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.6008214354515076, 'accuracy': 0.8070071339607239, 'precision': 0.8044758439064026, 'recall': 0.8111639022827148, 'f1_score': 0.8078060302422303}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:31:42.359018: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:31:42.436555: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:31:42.436653: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:31:42.438493: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:31:42.438607: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:31:42.438656: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:31:42.634170: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:31:42.634320: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:31:42.634347: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:31:42.634415: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:31:42.634447: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:32:28.173652: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:32:30.385205: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3301616e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:32:30.385272: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:32:30.391968: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717882350.465194    6078 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2024-06-08 18:33:01.583681: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:33:01.583747: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:33:01.620305: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:33:01.620375: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:33:01.660593: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:33:01.660672: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:33:01.697664: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:33:01.697730: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 6.55GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:33:01.755935: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:33:01.756005: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 5.92GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "[I 2024-06-08 18:33:39,076] Trial 46 finished with value: 0.7381235361099243 and parameters: {'learning_rate': 0.0009411476829867385, 'epsilon': 4.181804724101054e-06, 'maxlen': 1200, 'batch_size': 64}. Best is trial 36 with value: 0.815914511680603.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.5260790586471558, 'accuracy': 0.7381235361099243, 'precision': 0.6672226786613464, 'recall': 0.9501187801361084, 'f1_score': 0.7839294467851216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:33:39.687222: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:33:39.762375: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:33:39.762478: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:33:39.764267: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:33:39.764368: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:33:39.764411: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:33:39.956673: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:33:39.956798: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:33:39.956820: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:33:39.956878: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:33:39.956907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:34:03.431911: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:34:03.909236: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:03.971063: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:04.026912: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:04.071641: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:04.133784: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:04.191485: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:04.238548: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:04.281183: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:04.345372: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:04.401218: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:34:05.162102: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3300d8fb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:34:05.162146: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:34:05.168087: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717882445.238571    9326 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:35:49,847] Trial 47 finished with value: 0.8171021342277527 and parameters: {'learning_rate': 0.000954391452726391, 'epsilon': 5.2921581984712986e-06, 'maxlen': 600, 'batch_size': 56}. Best is trial 47 with value: 0.8171021342277527.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.4215407073497772, 'accuracy': 0.8171021342277527, 'precision': 0.7947019934654236, 'recall': 0.8551068902015686, 'f1_score': 0.823798631461814}\n",
      "Trial 47 achieved value: 0.8171021342277527 with  0.1453% improvement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:35:50.441506: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:35:50.514832: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:35:50.514930: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:35:50.527108: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:35:50.527247: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:35:50.527305: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:35:50.713487: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:35:50.713620: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:35:50.713642: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:35:50.713705: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:35:50.713732: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:36:14.505963: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:36:14.885145: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:14.955163: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:15.008995: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:15.049596: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:15.115042: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:15.169897: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:15.241243: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:15.305286: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:15.352499: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:15.391910: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:36:16.129887: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3309f28b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:36:16.129933: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:36:16.135953: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717882576.207041   13146 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:37:21,092] Trial 48 finished with value: 0.7719715237617493 and parameters: {'learning_rate': 0.0009324509502369932, 'epsilon': 4.545467792899217e-06, 'maxlen': 600, 'batch_size': 56}. Best is trial 47 with value: 0.8171021342277527.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.4835796654224396, 'accuracy': 0.7719715237617493, 'precision': 0.7059352397918701, 'recall': 0.9323040246963501, 'f1_score': 0.8034800282223072}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 18:37:21.728641: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:37:21.789429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:37:21.789527: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:37:21.791487: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:37:21.791592: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:37:21.791637: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:37:21.975177: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:37:21.975312: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:37:21.975334: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-06-08 18:37:21.975395: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-06-08 18:37:21.975424: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "2024-06-08 18:37:46.172740: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "2024-06-08 18:37:46.598983: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:46.647852: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:46.723195: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:46.782259: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:46.852414: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:46.917344: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:46.981836: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:47.038731: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:47.084713: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:47.124763: W external/local_tsl/tsl/framework/bfc_allocator.cc:296] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.29GiB with freed_by_count=0. The caller indicates that this is not a failure, but this may mean that there could be performance gains if more memory were available.\n",
      "2024-06-08 18:37:47.882620: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fd3241cbb00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-06-08 18:37:47.882663: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1660 Ti, Compute Capability 7.5\n",
      "2024-06-08 18:37:47.888309: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1717882667.960170   16735 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "[I 2024-06-08 18:39:33,735] Trial 49 finished with value: 0.820071280002594 and parameters: {'learning_rate': 0.0008946368929683414, 'epsilon': 6.581594777234862e-06, 'maxlen': 600, 'batch_size': 56}. Best is trial 49 with value: 0.820071280002594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "{'loss': 0.45601433515548706, 'accuracy': 0.820071280002594, 'precision': 0.8181818127632141, 'recall': 0.8230403661727905, 'f1_score': 0.820603898015944}\n",
      "Trial 49 achieved value: 0.820071280002594 with  0.3621% improvement\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id, run_name=\"CNN_MODEL_NO_AUG_SKIP\"):\n",
    "    # Initialize the Optuna study\n",
    "    pruner = optuna.pruners.HyperbandPruner(min_resource=2, max_resource=20, reduction_factor=3)\n",
    "    sampler=optuna.samplers.TPESampler(multivariate=True)\n",
    "    study_name = \"CNN_MODEL_NO_AUG_SKIP_V10\"  # Unique identifier of the study.\n",
    "    storage_name = \"sqlite:///{}.db\".format(study_name)\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=pruner,\n",
    "        sampler=sampler,\n",
    "        study_name=study_name,\n",
    "        storage=storage_name,\n",
    "        load_if_exists=True\n",
    "    )\n",
    "\n",
    "    # Execute the hyperparameter optimization trials.\n",
    "    # Note the addition of the `champion_callback` inclusion to control our logging\n",
    "    complete_trials = 0\n",
    "    while complete_trials < 50:\n",
    "        study.optimize(lambda x: objective(x, study), n_trials=1, callbacks=[champion_callback])\n",
    "        complete_trials = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
    "\n",
    "    # Log tags\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"FAKE_NEWS\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"CNN\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d520c-3ea4-4d98-a20c-de1872e7c401",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "#early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=4, verbose=1, mode=\"max\")\n",
    "#filepath = 'fake_model_cnn.h5'\n",
    "\n",
    "#checkpoint = ModelCheckpoint(filepath,monitor = 'val_accuracy', save_best_only=True, mode='max', save_weights_only=True)\n",
    "tf.keras.utils.set_random_seed(5113)\n",
    "callbacks_list = []\n",
    "def pad_split(x):\n",
    "        x += ((1200 - len(x)) if len(x)<1200 else 0) * [0]    \n",
    "        return x[:1200]\n",
    "train_cnn = train['train_token'].apply(lambda x: pad_split(x))\n",
    "validate_cnn = validate['train_token'].apply(lambda x: pad_split(x))\n",
    "\n",
    "import time, datetime\n",
    "model = get_model(0.0009424649455690445, 2.6200089221648253e-6, 1200, 6, 200)\n",
    "history=model.fit(list(train_cnn.values), [float(x) for x in train[\"misinformation\"].values],validation_data=(list(validate_cnn.values), [float(x) for x in validate[\"misinformation\"].values]), epochs=3,callbacks=callbacks_list,batch_size=56,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc2a7ff9-0844-42a5-aa30-22d3ee28a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pickle\n",
    "import nltk.data\n",
    "import itertools\n",
    "sent_tokenizer = nltk.data.load('tokenizers/punkt/PY3/portuguese.pickle')\n",
    "class FakeTokenizer:\n",
    "    def __init__(self, all_training_words, train, maxlen, sent_tokenizer):\n",
    "        self.sent_tokenizer = sent_tokenizer\n",
    "        self.T_VOCAB = sorted(list(all_training_words))\n",
    "        self.tokenizer = Tokenizer(num_words=len(T_VOCAB), char_level=False)\n",
    "        self.tokenizer.fit_on_texts(list(train[\"tokens\"].values))\n",
    "        self.maxlen = maxlen\n",
    "    def pad_split(self, x):\n",
    "        x += ((self.maxlen - len(x)) if len(x)<self.maxlen else 0) * [0]    \n",
    "        return x[:self.maxlen]\n",
    "    def text_tokenizer(self, text):\n",
    "        tokenized_text = self.sent_tokenizer.tokenize(text)\n",
    "        tokenized_text = self.tokenizer.texts_to_sequences(tokenized_text)\n",
    "        tokenized_text = list(itertools.chain.from_iterable(tokenized_text))\n",
    "        return self.pad_split(tokenized_text)\n",
    "    \n",
    "\n",
    "with open('fake_skip_no_aug_tokenizer_cnn.pkl', 'wb') as f:  # open a text file\n",
    "    pickle.dump(FakeTokenizer(all_training_words, train, 1200, sent_tokenizer), f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d7b0bec-6bdd-4ee9-ad04-896466a1fe4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('cnn_model_best.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1da89413-e08c-4186-99bd-893d4d38f258",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 04:16:49.528661: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-01 04:16:50.463405: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-01 04:16:50.463533: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-01 04:16:50.466256: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-01 04:16:50.466383: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-01 04:16:50.466429: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-01 04:16:50.725846: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-01 04:16:50.725950: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-01 04:16:50.725962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-07-01 04:16:50.726019: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-07-01 04:16:50.726040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4080 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1660 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "model = get_model(0.0008946368929683414, 6.581594777234862e-06, 600, 6, 100)\n",
    "model.load_weights('best_cnn_0.820071280002594.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4849e63c-4842-4e56-8a13-01ffa15b05b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>misinformation</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3722</td>\n",
       "      <td>O PT convocou todos os macumbeiros do Brasil e...</td>\n",
       "      <td>1</td>\n",
       "      <td>3722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16059</td>\n",
       "      <td>Bem, acredito que esteje tudo bem com voc, a...</td>\n",
       "      <td>1</td>\n",
       "      <td>16059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13247</td>\n",
       "      <td>Laudo mdico rompimento da artria devido o e...</td>\n",
       "      <td>1</td>\n",
       "      <td>13247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11255</td>\n",
       "      <td>J est sendo vendida em outros Estados e es...</td>\n",
       "      <td>1</td>\n",
       "      <td>11255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15979</td>\n",
       "      <td>Na Argentina ta tudo normal! STF da Argentina ...</td>\n",
       "      <td>1</td>\n",
       "      <td>15979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>6283</td>\n",
       "      <td>\\n\\nGeneral Paulo Chagas alerta Ciro Gomes\\n\\...</td>\n",
       "      <td>0</td>\n",
       "      <td>6283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>6707</td>\n",
       "      <td>moradores da cidade argentina santiago del est...</td>\n",
       "      <td>0</td>\n",
       "      <td>6707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>4081</td>\n",
       "      <td>Montes claros parou pra gritar 17.\\n s o com...</td>\n",
       "      <td>0</td>\n",
       "      <td>4081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>14727</td>\n",
       "      <td>A atriz da Globo teve um vdeo vazado na inter...</td>\n",
       "      <td>0</td>\n",
       "      <td>14727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>12575</td>\n",
       "      <td>Lanada nesta tera-feira (29) no Brasil com 5...</td>\n",
       "      <td>0</td>\n",
       "      <td>12575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1684 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                               text  \\\n",
       "0           3722  O PT convocou todos os macumbeiros do Brasil e...   \n",
       "1          16059  Bem, acredito que esteje tudo bem com voc, a...   \n",
       "2          13247  Laudo mdico rompimento da artria devido o e...   \n",
       "3          11255  J est sendo vendida em outros Estados e es...   \n",
       "4          15979  Na Argentina ta tudo normal! STF da Argentina ...   \n",
       "...          ...                                                ...   \n",
       "1679        6283  \\n\\nGeneral Paulo Chagas alerta Ciro Gomes\\n\\...   \n",
       "1680        6707  moradores da cidade argentina santiago del est...   \n",
       "1681        4081  Montes claros parou pra gritar 17.\\n s o com...   \n",
       "1682       14727  A atriz da Globo teve um vdeo vazado na inter...   \n",
       "1683       12575  Lanada nesta tera-feira (29) no Brasil com 5...   \n",
       "\n",
       "      misinformation  index  \n",
       "0                  1   3722  \n",
       "1                  1  16059  \n",
       "2                  1  13247  \n",
       "3                  1  11255  \n",
       "4                  1  15979  \n",
       "...              ...    ...  \n",
       "1679               0   6283  \n",
       "1680               0   6707  \n",
       "1681               0   4081  \n",
       "1682               0  14727  \n",
       "1683               0  12575  \n",
       "\n",
       "[1684 rows x 4 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"fake_test_balanced_indexed.csv\")\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "681971b1-dacc-45a5-8a28-3cbb570209f4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "z1p1DWoAJump",
    "outputId": "2f81ae69-a566-4890-fa8b-0222e741bd44"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-01 04:17:59.280320: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn import metrics\n",
    "from transformers import pipeline\n",
    "def classifier(text, model, tokenizer):\n",
    "    return model.predict([tokenizer(text)], verbose=0)\n",
    "#classifier = pipeline(task=\"text-classification\", model=model, top_k=None, tokenizer=FakeTokenizer(all_training_words, train, 1200).tokenizer)\n",
    "fake_tokenizer = FakeTokenizer(all_training_words, train, 600, sent_tokenizer)\n",
    "classify_all = test[\"text\"].apply(lambda x: classifier(x, model, fake_tokenizer.text_tokenizer)).values\n",
    "\n",
    "pred_labels = []\n",
    "for classification in classify_all:\n",
    "  if classification[0] >= 0.5:\n",
    "    pred_labels.append(1)\n",
    "  else:\n",
    "    pred_labels.append(0)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(test[\"misinformation\"].values, pred_labels)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"No Fake News\", \"Fake News\"])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1293134b-1bf9-44b3-9e43-18ba4c568877",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "id": "2jkMALsgc9Bj",
    "outputId": "09cea994-d3d2-42bf-d7cc-23d5a88e42e1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#old_test = pd.read_csv(\"drive/MyDrive/test_fake.csv\")\n",
    "##classifier = pipeline(task=\"text-classification\", model=model, top_k=None, tokenizer=tokenizer)\n",
    "#classify_all_old = classifier(list(old_test[\"text\"].values))\n",
    "#\n",
    "#pred_labels = []\n",
    "#for classification in classify_all_old:\n",
    "#  label0 = 0\n",
    "#  label1 = 0\n",
    "#  for label in classification:\n",
    "#    if label[\"label\"] == \"LABEL_0\":\n",
    "#      label0=label[\"score\"]\n",
    "#    if label[\"label\"] == \"LABEL_1\":\n",
    "#      label1=label[\"score\"]\n",
    "#  if label0 > label1:\n",
    "#    pred_labels.append(0)\n",
    "#  else:\n",
    "#    pred_labels.append(1)\n",
    "\n",
    "confusion_matrix = metrics.confusion_matrix(test[\"misinformation\"].values, pred_labels)\n",
    "\n",
    "cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix, display_labels = [\"No Fake News\", \"Fake News\"])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.savefig('cnn_results.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9881d52b-3032-4726-911e-a1fa82e9a6a4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hMqhIC2L-nDz",
    "outputId": "12b09ff9-1b69-4c30-8988-ce4110f44007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.79      0.81       842\n",
      "           1       0.80      0.83      0.81       842\n",
      "\n",
      "    accuracy                           0.81      1684\n",
      "   macro avg       0.81      0.81      0.81      1684\n",
      "weighted avg       0.81      0.81      0.81      1684\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = [\"0\", \"1\"]\n",
    "print(classification_report(test[\"misinformation\"].map(lambda x: str(x)).values, [ str(x) for x in pred_labels], target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77eb76c7-9db8-438d-bec2-385dcc807543",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_BCF_RLAOVR",
    "outputId": "8398d898-3387-4896-97a7-8c4992485e32"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.8208589, 0.8009206]),\n",
       " array([0.79453682, 0.82660333]),\n",
       " array([0.8074834 , 0.81355932]),\n",
       " array([842, 842]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.precision_recall_fscore_support(test[\"misinformation\"].values, pred_labels, average=None, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09125e49-8928-48f3-9f7f-6311c156fde5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-WhVwxg26Rx8",
    "outputId": "b13bbfed-a4d2-40cd-fbf4-c21f25b2388c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8105700712589073"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(test[\"misinformation\"].values, pred_labels, normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff152cd2-07bc-4cfc-b0d0-583388fe0a09",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 699
    },
    "id": "eVSrC3Ru0P0A",
    "outputId": "7dded4b7-62a9-448d-e609-0d248fc6626a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "with open(\"best_cnn_history.json\") as f:\n",
    "    history = json.loads(f.read())\n",
    "acc = history['accuracy']\n",
    "val_acc = history['val_accuracy']\n",
    "loss = history['loss']\n",
    "val_loss = history['val_loss']\n",
    "epochs_range = range(7)\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4fa45-0ed5-4c5d-9c8c-245efa8c3497",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
